scrapy框架

-- 什么是框架
    - 集成了很多功能的,并且具有很强通用性的一个项目模板.

-- 如何学习框架
    - 专门学习框架封装各种功能的详细用法.

-- 什么是scrapy?
    - 爬虫中封装好的一个明星框架.功能: 高性能的持久化操作,异步的数据下载,高性能的数据解析,分布式

- scrapy框架的基本使用
    - 环境的安装
        - mac&liunx  pip install scrapy
        - windows:
            pip install wheel
            下载Twisted 安装
            pip install pywin32
            conda install scrapy

    - 创建一个工程: scrapy startproject xxxPro
     - cd xxxPro
    - 需要在spiders子目录中创建一个爬虫文件
        - scrapy genspider spidername www.xxx.com

    - 执行工程
        - scrapy crawl spidername


- 数据解析 extract_first()
    content = div.xpath('./a[1]/div/span[1]//text()').extract()
- 持久化存储
    - 基于终端指令:
        - 要求: 只可以将parse方法的返回值存储到本地的文本文件
        - 注意; 持久化存储对应的文本文件类型只可以为 json csv pickle xml
        - 指令:  scrapy crawl xxx -o filePath      -o 存储路径
        - 好处: 简单,便捷
        - 缺点: 局限性比较强(数据只可以存储到指定后缀的文本文件)

    - 基于管道:
        - 编码流程:
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装储存到item类型的对象
            - 将item对象提交给管道进行持久化储存的操作
            - 在管道类的process_item中要将其接收受到的item对象中储存的数据进行持久化存储操作
            - 需要在配置文件中开启管道

        - 好处:
            - 通用性强.
        - 面试题: 将爬取到的数据一份存储到本地一份存储到数据库.如何实现?
            - 管道文件中的一个管道类对应是将数据存储到一种平台
            - 爬虫文件调交的item只会给管道文件中第一个被执行的管道类接收
            - precess_item中的 return item 表示将item传递给下一个即将被执行的管道类


- 基于Spider的全站数据爬取
    - 就是将网站中某板块下的全部页码对应的页面数据爬取和解析
    - 需求 爬取校花网的图片名称
    - 实现方法:
        - 将所有页面的url添加到starts_ulrs列表 (不推荐)
        - 自动手动进行请求发送 (推荐)
            -   手动请求发送
                    - yield scrapy.Request(url=new_url,callback=parse)

- 五大核心组件
    - spider
        - 产生url并对url请求发送
        - 使用parse进行数据解析
    - 管道
        - 持久化存储

    - 引擎
        - 中转站

        - 为什么要经过引擎
            - 用作为数据流处理
                拿到不同的数据流,就会知道该调用不同的方法
                比如引擎拿到response,就会知道调用parse方法了
            - 可以触发事务

    - 调度器
        - 组成
            - 过滤器: 对重复请求对象去重过滤
            - 队列: 经过去重的对象放在队列中
        - 对请求对象进行处理

    - 下载器
        - 负责数据下载  -> response


- 请求传参
    - 使用场景: 如果要爬取解析的数据,不在同一张页面. (深度爬取)
    - 需求: 爬取boss的岗位名称,岗位描述

    # 请求传参  meta = {}
    yield scrapy.Request(deltail_url, callback=self.parse_det,meta={'item':item})


- 图片数据爬取之ImagePipeline
    - 基于scrapy爬字符串类型的数据和爬取图片类型的数据区别?
        - 字符串:只需要xpath进行解析且提交管道进行持久化存储
        - 图片: xpath解析出img的src的属性值.单股的对图片地址发起请求获取图片二进制类型数据

    - 基于ImagePipeline:
        -只需要将img的src的属性进行解析,提交到管道,管道就会对图片的属性值就会请求,获取二进制数据且持久化存储
    - 需求:
        - 爬取站长素材的高清图片

    - 使用流程:
        - 数据解析 (图片的地址)
        - 将存储图片地址的item提交到指定的管道类
        - 在管道文件中自订制一个基于ImagePipeline的一个管道类
            - get_media_request()
            - file_path()
            - item_completed()
        在配置文件中
            - 指定图片存储的目录结构IMAGES_STORE = './imgs_xx'
            - 指定开启的管道

- 中间件
    - 引擎和下载器中有一个中间件
        - 下载中间件:
            作用: 批量的拦截所有的请求和响应
        - 拦截请求:
            - UA伪装
            - 代理ip的设定
        - 拦截响应:
            - 篡改响应数据,或者响应对象

    - 引擎和爬虫中有一个中间件
        - 爬虫中间件 应用场景小
        - 需求: 爬取网易新闻的数据(标题 对应的内容)
            - 1.通过网易新闻的首页解析出5大板块对应的详情页的url (没有动态加载)
            - 2.每一个板块对应的新闻标题都是动态加载 (动态加载)
            - 3.通过解析出每一条新闻详情页的url获取详情页面的页面源码,解析出新闻内容

